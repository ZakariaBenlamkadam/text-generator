{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:41.960497Z",
     "iopub.status.busy": "2024-05-20T18:19:41.959866Z",
     "iopub.status.idle": "2024-05-20T18:19:41.982255Z",
     "shell.execute_reply": "2024-05-20T18:19:41.981243Z",
     "shell.execute_reply.started": "2024-05-20T18:19:41.960466Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, \"r\", encoding='utf-8') as f:\n",
    "            data = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {path}\")\n",
    "        return None\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {path}: {e}\")\n",
    "        return None\n",
    "    return data\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dictionary to turn punctuation into a token.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation_mark||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation_mark||',\n",
    "        '?': '||question_mark||',\n",
    "        '(': '||left_parenthesis||',\n",
    "        ')': '||right_parenthesis||',\n",
    "        '--': '||dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        text (list of str): The text data as a list of words.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries: vocab_to_int and int_to_vocab.\n",
    "    \"\"\"\n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {idx: word for idx, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: idx for idx, word in int_to_vocab.items()}\n",
    "    \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def preprocess_and_save_data(text, token_lookup, create_lookup_tables, output_file):\n",
    "    \"\"\"\n",
    "    Preprocess Text Data and Save to File\n",
    "    \"\"\"\n",
    "    token_dict = token_lookup()\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(key, f' {token} ')\n",
    "\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text + list(SPECIAL_WORDS.values()))\n",
    "    int_text = [vocab_to_int[word] for word in text]\n",
    "\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump((int_text, vocab_to_int, int_to_vocab, token_dict), f)\n",
    "\n",
    "def load_preprocess(file_path):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training Data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return None\n",
    "    except IOError as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Define the path to your dataset\n",
    "dataset_path = '/kaggle/input/cap111/cap.txt'\n",
    "\n",
    "# Load the text data\n",
    "text = load_data(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:42.079314Z",
     "iopub.status.busy": "2024-05-20T18:19:42.078710Z",
     "iopub.status.idle": "2024-05-20T18:19:42.089866Z",
     "shell.execute_reply": "2024-05-20T18:19:42.088960Z",
     "shell.execute_reply.started": "2024-05-20T18:19:42.079288Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the split ratio\n",
    "validation_ratio = 0.1\n",
    "\n",
    "# Split the text data\n",
    "lines = text.split('\\n')\n",
    "split_index = int(len(lines) * (1 - validation_ratio))\n",
    "\n",
    "train_lines = lines[:split_index]\n",
    "val_lines = lines[split_index:]\n",
    "\n",
    "# Join lines back into single strings\n",
    "train_text = '\\n'.join(train_lines)\n",
    "val_text = '\\n'.join(val_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:42.212625Z",
     "iopub.status.busy": "2024-05-20T18:19:42.211942Z",
     "iopub.status.idle": "2024-05-20T18:19:42.360840Z",
     "shell.execute_reply": "2024-05-20T18:19:42.360074Z",
     "shell.execute_reply.started": "2024-05-20T18:19:42.212594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess and save the training data\n",
    "preprocess_and_save_data(train_text, token_lookup, create_lookup_tables, 'train_preprocess.p')\n",
    "\n",
    "# Preprocess and save the validation data\n",
    "preprocess_and_save_data(val_text, token_lookup, create_lookup_tables, 'val_preprocess.p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:42.369037Z",
     "iopub.status.busy": "2024-05-20T18:19:42.368403Z",
     "iopub.status.idle": "2024-05-20T18:19:42.391374Z",
     "shell.execute_reply": "2024-05-20T18:19:42.390658Z",
     "shell.execute_reply.started": "2024-05-20T18:19:42.369010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the preprocessed training data\n",
    "int_train_text, vocab_to_int, int_to_vocab, token_dict = load_preprocess('train_preprocess.p')\n",
    "\n",
    "# Load the preprocessed validation data\n",
    "int_val_text, _, _, _ = load_preprocess('val_preprocess.p')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch data üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:43.149525Z",
     "iopub.status.busy": "2024-05-20T18:19:43.148942Z",
     "iopub.status.idle": "2024-05-20T18:19:44.152798Z",
     "shell.execute_reply": "2024-05-20T18:19:44.152003Z",
     "shell.execute_reply.started": "2024-05-20T18:19:43.149494Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    \"\"\"\n",
    "    n_batches = len(words) // batch_size\n",
    "    words = words[:n_batches * batch_size]\n",
    "    \n",
    "    x, y = [], []\n",
    "    for idx in range(0, len(words) - sequence_length):\n",
    "        x.append(words[idx:idx+sequence_length])\n",
    "        y.append(words[idx+sequence_length])\n",
    "    \n",
    "    data = TensorDataset(torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y)))\n",
    "    data_loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "# Parameters for batching\n",
    "sequence_length = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Create the dataloader\n",
    "train_loader = batch_data(int_train_text, sequence_length, batch_size)\n",
    "val_loader = batch_data(int_val_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:44.154999Z",
     "iopub.status.busy": "2024-05-20T18:19:44.154707Z",
     "iopub.status.idle": "2024-05-20T18:19:44.167418Z",
     "shell.execute_reply": "2024-05-20T18:19:44.166539Z",
     "shell.execute_reply.started": "2024-05-20T18:19:44.154974Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim * 2, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_dim * 2, hidden_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        \"\"\"\n",
    "        h_lstm, c_lstm = hidden\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (h_lstm, c_lstm) = self.lstm1(x, (h_lstm, c_lstm))\n",
    "        lstm_out, (h_lstm, c_lstm) = self.lstm2(lstm_out, (h_lstm, c_lstm))\n",
    "        \n",
    "        gru_out, h_gru = self.gru(lstm_out, h_lstm)\n",
    "        \n",
    "        gru_out = self.dropout(gru_out)\n",
    "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim * 2)\n",
    "        out = self.fc(gru_out)\n",
    "        out = out.view(x.size(0), -1, self.output_size)\n",
    "        out = out[:, -1]\n",
    "        return out, (h_gru, c_lstm)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the hidden state\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        num_directions = 2  # Since the LSTM is bidirectional\n",
    "        h0 = weight.new(self.n_layers * num_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        c0 = weight.new(self.n_layers * num_directions, batch_size, self.hidden_dim).zero_().to(device)\n",
    "        return (h0, c0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the data üèãÔ∏è‚Äç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:44.169049Z",
     "iopub.status.busy": "2024-05-20T18:19:44.168795Z",
     "iopub.status.idle": "2024-05-20T18:19:44.180080Z",
     "shell.execute_reply": "2024-05-20T18:19:44.179263Z",
     "shell.execute_reply.started": "2024-05-20T18:19:44.169025Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden, clip=5):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    \"\"\"\n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    if torch.cuda.is_available():\n",
    "        inp, target = inp.cuda(), target.cuda()\n",
    "        \n",
    "    rnn.zero_grad()\n",
    "    output, hidden = rnn(inp, hidden)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), hidden\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "    \n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            if inputs.size(0) != batch_size:\n",
    "                continue  # Skip the last batch if it's not full size\n",
    "            \n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)\n",
    "            \n",
    "            batch_losses.append(loss)\n",
    "            \n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                avg_loss = np.mean(batch_losses)\n",
    "                print(f'Epoch: {epoch_i}/{n_epochs}, Batch: {batch_i}, Loss: {avg_loss}')\n",
    "                batch_losses = []\n",
    "    return rnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:19:44.461263Z",
     "iopub.status.busy": "2024-05-20T18:19:44.460649Z",
     "iopub.status.idle": "2024-05-20T18:19:44.468057Z",
     "shell.execute_reply": "2024-05-20T18:19:44.465770Z",
     "shell.execute_reply.started": "2024-05-20T18:19:44.461223Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(file_path, model):\n",
    "    torch.save(model.state_dict(), file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:30:16.351024Z",
     "iopub.status.busy": "2024-05-20T18:30:16.350401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Batch: 100, Loss: 6.526710863113403\n",
      "Epoch: 1/50, Batch: 200, Loss: 5.9580798959732055\n",
      "Epoch: 1/50, Batch: 300, Loss: 5.80269193649292\n",
      "Epoch: 1/50, Batch: 400, Loss: 5.695820107460022\n",
      "Epoch: 1/50, Batch: 500, Loss: 5.586495618820191\n",
      "Epoch: 1/50, Batch: 600, Loss: 5.431292552947998\n",
      "Epoch: 1/50, Batch: 700, Loss: 5.38152015209198\n",
      "Epoch: 1/50, Batch: 800, Loss: 5.299539566040039\n",
      "Epoch: 1/50, Batch: 900, Loss: 5.256376566886902\n",
      "Epoch: 1/50, Batch: 1000, Loss: 5.200288867950439\n",
      "Epoch: 1/50, Batch: 1100, Loss: 5.154879651069641\n",
      "Epoch: 1/50, Batch: 1200, Loss: 5.132998042106628\n",
      "Epoch: 1/50, Batch: 1300, Loss: 5.091118860244751\n",
      "Epoch: 1/50, Batch: 1400, Loss: 5.080178213119507\n",
      "Epoch: 1/50, Batch: 1500, Loss: 5.054933958053589\n",
      "Epoch: 1/50, Batch: 1600, Loss: 5.0320326566696165\n",
      "Epoch: 1/50, Batch: 1700, Loss: 5.005317163467407\n",
      "Epoch: 1/50, Batch: 1800, Loss: 4.981086397171021\n",
      "Epoch: 1/50, Batch: 1900, Loss: 4.926278576850891\n",
      "Epoch: 2/50, Batch: 100, Loss: 4.819538390741939\n",
      "Epoch: 2/50, Batch: 200, Loss: 4.7830656099319455\n",
      "Epoch: 2/50, Batch: 300, Loss: 4.763384766578675\n",
      "Epoch: 2/50, Batch: 400, Loss: 4.804715390205383\n",
      "Epoch: 2/50, Batch: 500, Loss: 4.768919091224671\n",
      "Epoch: 2/50, Batch: 600, Loss: 4.782433137893677\n",
      "Epoch: 2/50, Batch: 700, Loss: 4.7417447781562805\n",
      "Epoch: 2/50, Batch: 800, Loss: 4.741755194664002\n",
      "Epoch: 2/50, Batch: 900, Loss: 4.718777394294738\n",
      "Epoch: 2/50, Batch: 1000, Loss: 4.757382535934449\n",
      "Epoch: 2/50, Batch: 1100, Loss: 4.716039352416992\n",
      "Epoch: 2/50, Batch: 1200, Loss: 4.784121427536011\n",
      "Epoch: 2/50, Batch: 1300, Loss: 4.725290451049805\n",
      "Epoch: 2/50, Batch: 1400, Loss: 4.7282840085029605\n",
      "Epoch: 2/50, Batch: 1500, Loss: 4.7237609148025514\n",
      "Epoch: 2/50, Batch: 1600, Loss: 4.715867800712585\n",
      "Epoch: 2/50, Batch: 1700, Loss: 4.729054656028747\n",
      "Epoch: 2/50, Batch: 1800, Loss: 4.734873819351196\n",
      "Epoch: 2/50, Batch: 1900, Loss: 4.670382661819458\n",
      "Epoch: 3/50, Batch: 100, Loss: 4.525384829107639\n",
      "Epoch: 3/50, Batch: 200, Loss: 4.534309871196747\n",
      "Epoch: 3/50, Batch: 300, Loss: 4.466685032844543\n",
      "Epoch: 3/50, Batch: 400, Loss: 4.474000234603881\n",
      "Epoch: 3/50, Batch: 500, Loss: 4.468021233081817\n",
      "Epoch: 3/50, Batch: 600, Loss: 4.525997450351715\n",
      "Epoch: 3/50, Batch: 700, Loss: 4.504668006896972\n",
      "Epoch: 3/50, Batch: 800, Loss: 4.506204543113708\n",
      "Epoch: 3/50, Batch: 900, Loss: 4.504865872859955\n",
      "Epoch: 3/50, Batch: 1000, Loss: 4.527848210334778\n",
      "Epoch: 3/50, Batch: 1100, Loss: 4.560443227291107\n",
      "Epoch: 3/50, Batch: 1200, Loss: 4.517809481620788\n",
      "Epoch: 3/50, Batch: 1300, Loss: 4.465170302391052\n",
      "Epoch: 3/50, Batch: 1400, Loss: 4.501805038452148\n",
      "Epoch: 3/50, Batch: 1500, Loss: 4.468934342861176\n",
      "Epoch: 3/50, Batch: 1600, Loss: 4.524112629890442\n",
      "Epoch: 3/50, Batch: 1700, Loss: 4.474490792751312\n",
      "Epoch: 3/50, Batch: 1800, Loss: 4.5345216512680055\n",
      "Epoch: 3/50, Batch: 1900, Loss: 4.529724626541138\n",
      "Epoch: 4/50, Batch: 100, Loss: 4.311591971237047\n",
      "Epoch: 4/50, Batch: 200, Loss: 4.279090437889099\n",
      "Epoch: 4/50, Batch: 300, Loss: 4.2502516746521\n",
      "Epoch: 4/50, Batch: 400, Loss: 4.315017309188843\n",
      "Epoch: 4/50, Batch: 500, Loss: 4.293227727413178\n",
      "Epoch: 4/50, Batch: 600, Loss: 4.286382517814636\n",
      "Epoch: 4/50, Batch: 700, Loss: 4.3204412627220155\n",
      "Epoch: 4/50, Batch: 800, Loss: 4.347863645553589\n",
      "Epoch: 4/50, Batch: 900, Loss: 4.327038636207581\n",
      "Epoch: 4/50, Batch: 1000, Loss: 4.336308677196502\n",
      "Epoch: 4/50, Batch: 1100, Loss: 4.345955204963684\n",
      "Epoch: 4/50, Batch: 1200, Loss: 4.334104795455932\n",
      "Epoch: 4/50, Batch: 1300, Loss: 4.358640789985657\n",
      "Epoch: 4/50, Batch: 1400, Loss: 4.319150769710541\n",
      "Epoch: 4/50, Batch: 1500, Loss: 4.306607086658477\n",
      "Epoch: 4/50, Batch: 1600, Loss: 4.328327944278717\n",
      "Epoch: 4/50, Batch: 1700, Loss: 4.346254048347473\n",
      "Epoch: 4/50, Batch: 1800, Loss: 4.3213982629776\n",
      "Epoch: 4/50, Batch: 1900, Loss: 4.366492440700531\n",
      "Epoch: 5/50, Batch: 100, Loss: 4.121784798866879\n",
      "Epoch: 5/50, Batch: 200, Loss: 4.132578129768372\n",
      "Epoch: 5/50, Batch: 300, Loss: 4.156462168693542\n",
      "Epoch: 5/50, Batch: 400, Loss: 4.131483650207519\n",
      "Epoch: 5/50, Batch: 500, Loss: 4.141747393608093\n",
      "Epoch: 5/50, Batch: 600, Loss: 4.144171006679535\n",
      "Epoch: 5/50, Batch: 700, Loss: 4.144117915630341\n",
      "Epoch: 5/50, Batch: 800, Loss: 4.147873411178589\n",
      "Epoch: 5/50, Batch: 900, Loss: 4.141942367553711\n",
      "Epoch: 5/50, Batch: 1000, Loss: 4.132466039657593\n",
      "Epoch: 5/50, Batch: 1100, Loss: 4.178023900985718\n",
      "Epoch: 5/50, Batch: 1200, Loss: 4.176931612491607\n",
      "Epoch: 5/50, Batch: 1300, Loss: 4.199673655033112\n",
      "Epoch: 5/50, Batch: 1400, Loss: 4.1849579024314885\n",
      "Epoch: 5/50, Batch: 1500, Loss: 4.171518487930298\n",
      "Epoch: 5/50, Batch: 1600, Loss: 4.184860029220581\n",
      "Epoch: 5/50, Batch: 1700, Loss: 4.2020739698410035\n",
      "Epoch: 5/50, Batch: 1800, Loss: 4.165735342502594\n",
      "Epoch: 5/50, Batch: 1900, Loss: 4.19013659954071\n",
      "Epoch: 6/50, Batch: 100, Loss: 3.9688877574110455\n",
      "Epoch: 6/50, Batch: 200, Loss: 3.9157685089111327\n",
      "Epoch: 6/50, Batch: 300, Loss: 3.9613730454444886\n",
      "Epoch: 6/50, Batch: 400, Loss: 3.9852021646499636\n",
      "Epoch: 6/50, Batch: 500, Loss: 3.980449652671814\n",
      "Epoch: 6/50, Batch: 600, Loss: 3.9737682795524596\n",
      "Epoch: 6/50, Batch: 700, Loss: 4.010220925807953\n",
      "Epoch: 6/50, Batch: 800, Loss: 4.001745228767395\n",
      "Epoch: 6/50, Batch: 900, Loss: 3.976055324077606\n",
      "Epoch: 6/50, Batch: 1000, Loss: 4.040154166221619\n",
      "Epoch: 6/50, Batch: 1100, Loss: 4.026455659866333\n",
      "Epoch: 6/50, Batch: 1200, Loss: 4.002856798171997\n",
      "Epoch: 6/50, Batch: 1300, Loss: 4.022888600826263\n",
      "Epoch: 6/50, Batch: 1400, Loss: 3.9957300090789794\n",
      "Epoch: 6/50, Batch: 1500, Loss: 4.051667423248291\n",
      "Epoch: 6/50, Batch: 1600, Loss: 4.131494927406311\n",
      "Epoch: 6/50, Batch: 1700, Loss: 4.084908194541931\n",
      "Epoch: 6/50, Batch: 1800, Loss: 4.030500752925873\n",
      "Epoch: 6/50, Batch: 1900, Loss: 4.046761920452118\n",
      "Epoch: 7/50, Batch: 100, Loss: 3.7924650939164963\n",
      "Epoch: 7/50, Batch: 200, Loss: 3.7607559251785276\n",
      "Epoch: 7/50, Batch: 300, Loss: 3.834551224708557\n",
      "Epoch: 7/50, Batch: 400, Loss: 3.821462559700012\n",
      "Epoch: 7/50, Batch: 500, Loss: 3.8150189328193664\n",
      "Epoch: 7/50, Batch: 600, Loss: 3.8378033781051637\n",
      "Epoch: 7/50, Batch: 700, Loss: 3.895792691707611\n",
      "Epoch: 7/50, Batch: 800, Loss: 3.8600432085990906\n",
      "Epoch: 7/50, Batch: 900, Loss: 3.875446753501892\n",
      "Epoch: 7/50, Batch: 1000, Loss: 3.828380973339081\n",
      "Epoch: 7/50, Batch: 1100, Loss: 3.8931076383590697\n",
      "Epoch: 7/50, Batch: 1200, Loss: 3.8675639295578\n",
      "Epoch: 7/50, Batch: 1300, Loss: 3.8960776257514955\n",
      "Epoch: 7/50, Batch: 1400, Loss: 3.878003761768341\n",
      "Epoch: 7/50, Batch: 1500, Loss: 3.9350058889389037\n",
      "Epoch: 7/50, Batch: 1600, Loss: 3.939019992351532\n",
      "Epoch: 7/50, Batch: 1700, Loss: 3.9306999182701112\n",
      "Epoch: 7/50, Batch: 1800, Loss: 3.9601257729530333\n",
      "Epoch: 7/50, Batch: 1900, Loss: 3.8984719777107237\n",
      "Epoch: 8/50, Batch: 100, Loss: 3.6698335989386632\n",
      "Epoch: 8/50, Batch: 200, Loss: 3.639241089820862\n",
      "Epoch: 8/50, Batch: 300, Loss: 3.6799408268928526\n",
      "Epoch: 8/50, Batch: 400, Loss: 3.650103073120117\n",
      "Epoch: 8/50, Batch: 500, Loss: 3.636791474819183\n",
      "Epoch: 8/50, Batch: 600, Loss: 3.6704344367980957\n",
      "Epoch: 8/50, Batch: 700, Loss: 3.6762053084373476\n",
      "Epoch: 8/50, Batch: 800, Loss: 3.7292082142829894\n",
      "Epoch: 8/50, Batch: 900, Loss: 3.7105609488487246\n",
      "Epoch: 8/50, Batch: 1000, Loss: 3.721463384628296\n",
      "Epoch: 8/50, Batch: 1100, Loss: 3.7632725286483764\n",
      "Epoch: 8/50, Batch: 1200, Loss: 3.7884682703018187\n",
      "Epoch: 8/50, Batch: 1300, Loss: 3.7773287987709043\n",
      "Epoch: 8/50, Batch: 1400, Loss: 3.7672858357429506\n",
      "Epoch: 8/50, Batch: 1500, Loss: 3.773291811943054\n",
      "Epoch: 8/50, Batch: 1600, Loss: 3.7986421823501586\n",
      "Epoch: 8/50, Batch: 1700, Loss: 3.8287048387527465\n",
      "Epoch: 8/50, Batch: 1800, Loss: 3.834159836769104\n",
      "Epoch: 8/50, Batch: 1900, Loss: 3.818446600437164\n",
      "Epoch: 9/50, Batch: 100, Loss: 3.4708841015807295\n",
      "Epoch: 9/50, Batch: 200, Loss: 3.511770794391632\n",
      "Epoch: 9/50, Batch: 300, Loss: 3.5097190070152284\n",
      "Epoch: 9/50, Batch: 400, Loss: 3.4991505765914916\n",
      "Epoch: 9/50, Batch: 500, Loss: 3.4994403076171876\n",
      "Epoch: 9/50, Batch: 600, Loss: 3.5788646507263184\n",
      "Epoch: 9/50, Batch: 700, Loss: 3.571434257030487\n",
      "Epoch: 9/50, Batch: 800, Loss: 3.569005165100098\n",
      "Epoch: 9/50, Batch: 900, Loss: 3.605878553390503\n",
      "Epoch: 9/50, Batch: 1000, Loss: 3.639651684761047\n",
      "Epoch: 9/50, Batch: 1100, Loss: 3.591359512805939\n",
      "Epoch: 9/50, Batch: 1200, Loss: 3.6381885528564455\n",
      "Epoch: 9/50, Batch: 1300, Loss: 3.6707771897315977\n",
      "Epoch: 9/50, Batch: 1400, Loss: 3.6495539712905884\n",
      "Epoch: 9/50, Batch: 1500, Loss: 3.6865464735031126\n",
      "Epoch: 9/50, Batch: 1600, Loss: 3.685552034378052\n",
      "Epoch: 9/50, Batch: 1700, Loss: 3.618880431652069\n",
      "Epoch: 9/50, Batch: 1800, Loss: 3.7029451060295107\n",
      "Epoch: 9/50, Batch: 1900, Loss: 3.7034335947036743\n",
      "Epoch: 10/50, Batch: 100, Loss: 3.3686883660544336\n",
      "Epoch: 10/50, Batch: 200, Loss: 3.334163661003113\n",
      "Epoch: 10/50, Batch: 300, Loss: 3.381090397834778\n",
      "Epoch: 10/50, Batch: 400, Loss: 3.3922023963928223\n",
      "Epoch: 10/50, Batch: 500, Loss: 3.3802384400367735\n",
      "Epoch: 10/50, Batch: 600, Loss: 3.420734820365906\n",
      "Epoch: 10/50, Batch: 700, Loss: 3.451110541820526\n",
      "Epoch: 10/50, Batch: 800, Loss: 3.4539447379112245\n",
      "Epoch: 10/50, Batch: 900, Loss: 3.44903879404068\n",
      "Epoch: 10/50, Batch: 1000, Loss: 3.501913685798645\n",
      "Epoch: 10/50, Batch: 1100, Loss: 3.5093566370010376\n",
      "Epoch: 10/50, Batch: 1200, Loss: 3.547369122505188\n",
      "Epoch: 10/50, Batch: 1300, Loss: 3.54293839931488\n",
      "Epoch: 10/50, Batch: 1400, Loss: 3.5167100954055788\n",
      "Epoch: 10/50, Batch: 1500, Loss: 3.5600293111801147\n",
      "Epoch: 10/50, Batch: 1600, Loss: 3.5796127271652223\n",
      "Epoch: 10/50, Batch: 1700, Loss: 3.562624070644379\n",
      "Epoch: 10/50, Batch: 1800, Loss: 3.5269125866889954\n",
      "Epoch: 10/50, Batch: 1900, Loss: 3.533301842212677\n",
      "Epoch: 11/50, Batch: 100, Loss: 3.2162584646613195\n",
      "Epoch: 11/50, Batch: 200, Loss: 3.223731606006622\n",
      "Epoch: 11/50, Batch: 300, Loss: 3.251590609550476\n",
      "Epoch: 11/50, Batch: 400, Loss: 3.2856744027137754\n",
      "Epoch: 11/50, Batch: 500, Loss: 3.276148006916046\n",
      "Epoch: 11/50, Batch: 600, Loss: 3.329488353729248\n",
      "Epoch: 11/50, Batch: 700, Loss: 3.3390296459198\n",
      "Epoch: 11/50, Batch: 800, Loss: 3.3112208104133605\n",
      "Epoch: 11/50, Batch: 900, Loss: 3.3557084465026854\n",
      "Epoch: 11/50, Batch: 1000, Loss: 3.3727520966529845\n",
      "Epoch: 11/50, Batch: 1100, Loss: 3.374260206222534\n",
      "Epoch: 11/50, Batch: 1200, Loss: 3.3948563623428343\n",
      "Epoch: 11/50, Batch: 1300, Loss: 3.38154821395874\n",
      "Epoch: 11/50, Batch: 1400, Loss: 3.3929421710968017\n",
      "Epoch: 11/50, Batch: 1500, Loss: 3.391094753742218\n",
      "Epoch: 11/50, Batch: 1600, Loss: 3.4154725313186645\n",
      "Epoch: 11/50, Batch: 1700, Loss: 3.4333054566383363\n",
      "Epoch: 11/50, Batch: 1800, Loss: 3.4368147253990173\n",
      "Epoch: 11/50, Batch: 1900, Loss: 3.484697606563568\n",
      "Epoch: 12/50, Batch: 100, Loss: 3.12018658418571\n",
      "Epoch: 12/50, Batch: 200, Loss: 3.101073763370514\n",
      "Epoch: 12/50, Batch: 300, Loss: 3.1195009660720827\n",
      "Epoch: 12/50, Batch: 400, Loss: 3.1675066781044006\n",
      "Epoch: 12/50, Batch: 500, Loss: 3.146261191368103\n",
      "Epoch: 12/50, Batch: 600, Loss: 3.144527406692505\n",
      "Epoch: 12/50, Batch: 700, Loss: 3.1760006880760194\n",
      "Epoch: 12/50, Batch: 800, Loss: 3.2263736438751223\n",
      "Epoch: 12/50, Batch: 900, Loss: 3.2480830764770507\n",
      "Epoch: 12/50, Batch: 1000, Loss: 3.258108160495758\n",
      "Epoch: 12/50, Batch: 1100, Loss: 3.295782084465027\n",
      "Epoch: 12/50, Batch: 1200, Loss: 3.270184562206268\n",
      "Epoch: 12/50, Batch: 1300, Loss: 3.296024968624115\n",
      "Epoch: 12/50, Batch: 1400, Loss: 3.2674062919616698\n",
      "Epoch: 12/50, Batch: 1500, Loss: 3.353210644721985\n",
      "Epoch: 12/50, Batch: 1600, Loss: 3.299512825012207\n",
      "Epoch: 12/50, Batch: 1700, Loss: 3.3134997200965883\n",
      "Epoch: 12/50, Batch: 1800, Loss: 3.337436852455139\n",
      "Epoch: 12/50, Batch: 1900, Loss: 3.3294536900520324\n",
      "Epoch: 13/50, Batch: 100, Loss: 2.9464004925921956\n",
      "Epoch: 13/50, Batch: 200, Loss: 2.9520257663726808\n",
      "Epoch: 13/50, Batch: 300, Loss: 2.9566698241233826\n",
      "Epoch: 13/50, Batch: 400, Loss: 3.031823101043701\n",
      "Epoch: 13/50, Batch: 500, Loss: 3.0474664855003355\n",
      "Epoch: 13/50, Batch: 600, Loss: 3.074116907119751\n",
      "Epoch: 13/50, Batch: 700, Loss: 3.113680810928345\n",
      "Epoch: 13/50, Batch: 800, Loss: 3.1136613154411314\n",
      "Epoch: 13/50, Batch: 900, Loss: 3.0948959231376647\n",
      "Epoch: 13/50, Batch: 1000, Loss: 3.1541248059272764\n",
      "Epoch: 13/50, Batch: 1100, Loss: 3.15133651971817\n",
      "Epoch: 13/50, Batch: 1200, Loss: 3.1911125874519346\n",
      "Epoch: 13/50, Batch: 1300, Loss: 3.2161806225776672\n",
      "Epoch: 13/50, Batch: 1400, Loss: 3.2089389181137085\n",
      "Epoch: 13/50, Batch: 1500, Loss: 3.208302233219147\n",
      "Epoch: 13/50, Batch: 1600, Loss: 3.161477150917053\n",
      "Epoch: 13/50, Batch: 1700, Loss: 3.190699028968811\n",
      "Epoch: 13/50, Batch: 1800, Loss: 3.219558348655701\n",
      "Epoch: 13/50, Batch: 1900, Loss: 3.217252118587494\n",
      "Epoch: 14/50, Batch: 100, Loss: 2.8276873837530085\n",
      "Epoch: 14/50, Batch: 200, Loss: 2.833901424407959\n",
      "Epoch: 14/50, Batch: 300, Loss: 2.843166375160217\n",
      "Epoch: 14/50, Batch: 400, Loss: 2.936630253791809\n",
      "Epoch: 14/50, Batch: 500, Loss: 2.9186427712440492\n",
      "Epoch: 14/50, Batch: 600, Loss: 2.932557158470154\n",
      "Epoch: 14/50, Batch: 700, Loss: 2.96004878282547\n",
      "Epoch: 14/50, Batch: 800, Loss: 2.9914409637451174\n",
      "Epoch: 14/50, Batch: 900, Loss: 2.980672297477722\n",
      "Epoch: 14/50, Batch: 1000, Loss: 2.996040620803833\n",
      "Epoch: 14/50, Batch: 1100, Loss: 3.053171594142914\n",
      "Epoch: 14/50, Batch: 1200, Loss: 3.0638624739646914\n",
      "Epoch: 14/50, Batch: 1300, Loss: 3.063697280883789\n",
      "Epoch: 14/50, Batch: 1400, Loss: 3.1005887722969057\n",
      "Epoch: 14/50, Batch: 1500, Loss: 3.1490671491622924\n",
      "Epoch: 14/50, Batch: 1600, Loss: 3.0838603496551515\n",
      "Epoch: 14/50, Batch: 1700, Loss: 3.133479516506195\n",
      "Epoch: 14/50, Batch: 1800, Loss: 3.1347610449790952\n",
      "Epoch: 14/50, Batch: 1900, Loss: 3.1954122734069825\n",
      "Epoch: 15/50, Batch: 100, Loss: 2.7510956262065247\n",
      "Epoch: 15/50, Batch: 200, Loss: 2.762039520740509\n",
      "Epoch: 15/50, Batch: 300, Loss: 2.739633574485779\n",
      "Epoch: 15/50, Batch: 400, Loss: 2.805789270401001\n",
      "Epoch: 15/50, Batch: 500, Loss: 2.798240449428558\n",
      "Epoch: 15/50, Batch: 600, Loss: 2.874241921901703\n",
      "Epoch: 15/50, Batch: 700, Loss: 2.850508861541748\n",
      "Epoch: 15/50, Batch: 800, Loss: 2.8657744240760805\n",
      "Epoch: 15/50, Batch: 900, Loss: 2.8888156056404113\n",
      "Epoch: 15/50, Batch: 1000, Loss: 2.9164499115943907\n",
      "Epoch: 15/50, Batch: 1100, Loss: 2.914949743747711\n",
      "Epoch: 15/50, Batch: 1200, Loss: 2.934850261211395\n",
      "Epoch: 15/50, Batch: 1300, Loss: 2.9569885396957396\n",
      "Epoch: 15/50, Batch: 1400, Loss: 3.000631456375122\n",
      "Epoch: 15/50, Batch: 1500, Loss: 2.994058036804199\n",
      "Epoch: 15/50, Batch: 1600, Loss: 2.935073974132538\n",
      "Epoch: 15/50, Batch: 1700, Loss: 2.992253453731537\n",
      "Epoch: 15/50, Batch: 1800, Loss: 3.065888452529907\n",
      "Epoch: 15/50, Batch: 1900, Loss: 3.057553753852844\n",
      "Epoch: 16/50, Batch: 100, Loss: 2.6482106710957214\n",
      "Epoch: 16/50, Batch: 200, Loss: 2.606429998874664\n",
      "Epoch: 16/50, Batch: 300, Loss: 2.6593080663681032\n",
      "Epoch: 16/50, Batch: 400, Loss: 2.7017902898788453\n",
      "Epoch: 16/50, Batch: 500, Loss: 2.7087755227088928\n",
      "Epoch: 16/50, Batch: 600, Loss: 2.715036406517029\n",
      "Epoch: 16/50, Batch: 700, Loss: 2.7665956521034243\n",
      "Epoch: 16/50, Batch: 800, Loss: 2.8054397654533387\n",
      "Epoch: 16/50, Batch: 900, Loss: 2.824636878967285\n",
      "Epoch: 16/50, Batch: 1000, Loss: 2.803849036693573\n",
      "Epoch: 16/50, Batch: 1100, Loss: 2.8258316564559935\n",
      "Epoch: 16/50, Batch: 1200, Loss: 2.826614811420441\n",
      "Epoch: 16/50, Batch: 1300, Loss: 2.862558481693268\n",
      "Epoch: 16/50, Batch: 1400, Loss: 2.878295261859894\n",
      "Epoch: 16/50, Batch: 1500, Loss: 2.8979625940322875\n",
      "Epoch: 16/50, Batch: 1600, Loss: 2.875987057685852\n",
      "Epoch: 16/50, Batch: 1700, Loss: 2.873640151023865\n",
      "Epoch: 16/50, Batch: 1800, Loss: 2.9215996932983397\n",
      "Epoch: 16/50, Batch: 1900, Loss: 2.939823532104492\n",
      "Epoch: 17/50, Batch: 100, Loss: 2.5375779223653065\n",
      "Epoch: 17/50, Batch: 200, Loss: 2.514911775588989\n",
      "Epoch: 17/50, Batch: 300, Loss: 2.541940038204193\n",
      "Epoch: 17/50, Batch: 400, Loss: 2.5608157300949097\n",
      "Epoch: 17/50, Batch: 500, Loss: 2.5744237303733826\n",
      "Epoch: 17/50, Batch: 600, Loss: 2.627589166164398\n",
      "Epoch: 17/50, Batch: 700, Loss: 2.6873818254470825\n",
      "Epoch: 17/50, Batch: 800, Loss: 2.6435838508605958\n",
      "Epoch: 17/50, Batch: 900, Loss: 2.68237250328064\n",
      "Epoch: 17/50, Batch: 1000, Loss: 2.718110146522522\n",
      "Epoch: 17/50, Batch: 1100, Loss: 2.723534469604492\n",
      "Epoch: 17/50, Batch: 1200, Loss: 2.7351069831848145\n",
      "Epoch: 17/50, Batch: 1300, Loss: 2.7855315661430358\n",
      "Epoch: 17/50, Batch: 1400, Loss: 2.819475417137146\n",
      "Epoch: 17/50, Batch: 1500, Loss: 2.7856557726860047\n",
      "Epoch: 17/50, Batch: 1600, Loss: 2.796991846561432\n",
      "Epoch: 17/50, Batch: 1700, Loss: 2.804936864376068\n",
      "Epoch: 17/50, Batch: 1800, Loss: 2.840795736312866\n",
      "Epoch: 17/50, Batch: 1900, Loss: 2.821665322780609\n",
      "Epoch: 18/50, Batch: 100, Loss: 2.4483831414079242\n",
      "Epoch: 18/50, Batch: 200, Loss: 2.4198014521598816\n",
      "Epoch: 18/50, Batch: 300, Loss: 2.4932887709140776\n",
      "Epoch: 18/50, Batch: 400, Loss: 2.481080722808838\n",
      "Epoch: 18/50, Batch: 500, Loss: 2.5027968001365664\n",
      "Epoch: 18/50, Batch: 600, Loss: 2.556693375110626\n",
      "Epoch: 18/50, Batch: 700, Loss: 2.524444253444672\n",
      "Epoch: 18/50, Batch: 800, Loss: 2.5772394979000093\n",
      "Epoch: 18/50, Batch: 900, Loss: 2.596565489768982\n",
      "Epoch: 18/50, Batch: 1000, Loss: 2.6250348353385924\n",
      "Epoch: 18/50, Batch: 1100, Loss: 2.640056791305542\n",
      "Epoch: 18/50, Batch: 1200, Loss: 2.6625967454910278\n",
      "Epoch: 18/50, Batch: 1300, Loss: 2.6676060128211976\n",
      "Epoch: 18/50, Batch: 1400, Loss: 2.6701666378974913\n",
      "Epoch: 18/50, Batch: 1500, Loss: 2.713486475944519\n",
      "Epoch: 18/50, Batch: 1600, Loss: 2.682183289527893\n",
      "Epoch: 18/50, Batch: 1700, Loss: 2.737953817844391\n",
      "Epoch: 18/50, Batch: 1800, Loss: 2.7469750881195067\n",
      "Epoch: 18/50, Batch: 1900, Loss: 2.720580983161926\n",
      "Epoch: 19/50, Batch: 100, Loss: 2.3801433080065566\n",
      "Epoch: 19/50, Batch: 200, Loss: 2.3267625045776366\n",
      "Epoch: 19/50, Batch: 300, Loss: 2.3650362968444822\n",
      "Epoch: 19/50, Batch: 400, Loss: 2.345013003349304\n",
      "Epoch: 19/50, Batch: 500, Loss: 2.4158438313007355\n",
      "Epoch: 19/50, Batch: 600, Loss: 2.4291874527931214\n",
      "Epoch: 19/50, Batch: 700, Loss: 2.4699523890018464\n",
      "Epoch: 19/50, Batch: 800, Loss: 2.5298734629154205\n",
      "Epoch: 19/50, Batch: 900, Loss: 2.508764429092407\n",
      "Epoch: 19/50, Batch: 1000, Loss: 2.504960935115814\n",
      "Epoch: 19/50, Batch: 1100, Loss: 2.512378144264221\n",
      "Epoch: 19/50, Batch: 1200, Loss: 2.5795159411430357\n",
      "Epoch: 19/50, Batch: 1300, Loss: 2.6135674905776978\n",
      "Epoch: 19/50, Batch: 1400, Loss: 2.59005806684494\n",
      "Epoch: 19/50, Batch: 1500, Loss: 2.574716718196869\n",
      "Epoch: 19/50, Batch: 1600, Loss: 2.6289015364646913\n",
      "Epoch: 19/50, Batch: 1700, Loss: 2.6437302911281586\n",
      "Epoch: 19/50, Batch: 1800, Loss: 2.6536690163612366\n",
      "Epoch: 19/50, Batch: 1900, Loss: 2.6319887804985047\n",
      "Epoch: 20/50, Batch: 100, Loss: 2.250220798813136\n",
      "Epoch: 20/50, Batch: 200, Loss: 2.256558600664139\n",
      "Epoch: 20/50, Batch: 300, Loss: 2.2353938841819763\n",
      "Epoch: 20/50, Batch: 400, Loss: 2.3165635311603547\n",
      "Epoch: 20/50, Batch: 500, Loss: 2.2922063517570495\n",
      "Epoch: 20/50, Batch: 600, Loss: 2.346015056371689\n",
      "Epoch: 20/50, Batch: 700, Loss: 2.3939818155765535\n",
      "Epoch: 20/50, Batch: 800, Loss: 2.4107538688182832\n",
      "Epoch: 20/50, Batch: 900, Loss: 2.438566848039627\n",
      "Epoch: 20/50, Batch: 1000, Loss: 2.4376223516464233\n",
      "Epoch: 20/50, Batch: 1100, Loss: 2.464335347414017\n",
      "Epoch: 20/50, Batch: 1200, Loss: 2.4736604118347167\n",
      "Epoch: 20/50, Batch: 1300, Loss: 2.4902199804782867\n",
      "Epoch: 20/50, Batch: 1400, Loss: 2.545250723361969\n",
      "Epoch: 20/50, Batch: 1500, Loss: 2.496398591995239\n",
      "Epoch: 20/50, Batch: 1600, Loss: 2.569071035385132\n",
      "Epoch: 20/50, Batch: 1700, Loss: 2.512131414413452\n",
      "Epoch: 20/50, Batch: 1800, Loss: 2.5725098061561584\n",
      "Epoch: 20/50, Batch: 1900, Loss: 2.5466903698444368\n",
      "Epoch: 21/50, Batch: 100, Loss: 2.201034283216021\n",
      "Epoch: 21/50, Batch: 200, Loss: 2.1696204352378845\n",
      "Epoch: 21/50, Batch: 300, Loss: 2.134721044301987\n",
      "Epoch: 21/50, Batch: 400, Loss: 2.21590052485466\n",
      "Epoch: 21/50, Batch: 500, Loss: 2.2465059673786163\n",
      "Epoch: 21/50, Batch: 600, Loss: 2.265757476091385\n",
      "Epoch: 21/50, Batch: 700, Loss: 2.3023979341983796\n",
      "Epoch: 21/50, Batch: 800, Loss: 2.3144987988471986\n",
      "Epoch: 21/50, Batch: 900, Loss: 2.3196925806999205\n",
      "Epoch: 21/50, Batch: 1000, Loss: 2.3499266934394836\n",
      "Epoch: 21/50, Batch: 1100, Loss: 2.380895303487778\n",
      "Epoch: 21/50, Batch: 1200, Loss: 2.3741547513008117\n",
      "Epoch: 21/50, Batch: 1300, Loss: 2.3620970261096956\n",
      "Epoch: 21/50, Batch: 1400, Loss: 2.4356415724754332\n",
      "Epoch: 21/50, Batch: 1500, Loss: 2.4481643891334532\n",
      "Epoch: 21/50, Batch: 1600, Loss: 2.456265411376953\n",
      "Epoch: 21/50, Batch: 1700, Loss: 2.4348335230350493\n",
      "Epoch: 21/50, Batch: 1800, Loss: 2.469188163280487\n",
      "Epoch: 21/50, Batch: 1900, Loss: 2.5237396502494813\n",
      "Epoch: 22/50, Batch: 100, Loss: 2.0907239808445484\n",
      "Epoch: 22/50, Batch: 200, Loss: 2.074793939590454\n",
      "Epoch: 22/50, Batch: 300, Loss: 2.0789705538749694\n",
      "Epoch: 22/50, Batch: 400, Loss: 2.1479319083690642\n",
      "Epoch: 22/50, Batch: 500, Loss: 2.179241063594818\n",
      "Epoch: 22/50, Batch: 600, Loss: 2.2057424759864808\n",
      "Epoch: 22/50, Batch: 700, Loss: 2.2388471364974976\n",
      "Epoch: 22/50, Batch: 800, Loss: 2.1930829668045044\n",
      "Epoch: 22/50, Batch: 900, Loss: 2.231151769161224\n",
      "Epoch: 22/50, Batch: 1000, Loss: 2.2431185841560364\n",
      "Epoch: 22/50, Batch: 1100, Loss: 2.2986548578739168\n",
      "Epoch: 22/50, Batch: 1200, Loss: 2.3277579724788664\n",
      "Epoch: 22/50, Batch: 1300, Loss: 2.336534721851349\n",
      "Epoch: 22/50, Batch: 1400, Loss: 2.346604905128479\n",
      "Epoch: 22/50, Batch: 1500, Loss: 2.380722506046295\n",
      "Epoch: 22/50, Batch: 1600, Loss: 2.369171370267868\n",
      "Epoch: 22/50, Batch: 1700, Loss: 2.35915558218956\n",
      "Epoch: 22/50, Batch: 1800, Loss: 2.424830003976822\n",
      "Epoch: 22/50, Batch: 1900, Loss: 2.3887871646881105\n",
      "Epoch: 23/50, Batch: 100, Loss: 1.9948541020925066\n",
      "Epoch: 23/50, Batch: 200, Loss: 2.0132288515567778\n",
      "Epoch: 23/50, Batch: 300, Loss: 1.987587206363678\n",
      "Epoch: 23/50, Batch: 400, Loss: 2.082871960401535\n",
      "Epoch: 23/50, Batch: 500, Loss: 2.089819746017456\n",
      "Epoch: 23/50, Batch: 600, Loss: 2.080425287485123\n",
      "Epoch: 23/50, Batch: 700, Loss: 2.1562791192531585\n",
      "Epoch: 23/50, Batch: 800, Loss: 2.1461888265609743\n",
      "Epoch: 23/50, Batch: 900, Loss: 2.1822511172294616\n",
      "Epoch: 23/50, Batch: 1000, Loss: 2.190407543182373\n",
      "Epoch: 23/50, Batch: 1100, Loss: 2.2268436992168428\n",
      "Epoch: 23/50, Batch: 1200, Loss: 2.2544887971878054\n",
      "Epoch: 23/50, Batch: 1300, Loss: 2.2625012791156767\n",
      "Epoch: 23/50, Batch: 1400, Loss: 2.293958728313446\n",
      "Epoch: 23/50, Batch: 1500, Loss: 2.297329910993576\n",
      "Epoch: 23/50, Batch: 1600, Loss: 2.298834857940674\n",
      "Epoch: 23/50, Batch: 1700, Loss: 2.322119354009628\n",
      "Epoch: 23/50, Batch: 1800, Loss: 2.310584570169449\n",
      "Epoch: 23/50, Batch: 1900, Loss: 2.361015484333038\n",
      "Epoch: 24/50, Batch: 100, Loss: 1.9659070525549154\n",
      "Epoch: 24/50, Batch: 200, Loss: 1.9452056515216827\n",
      "Epoch: 24/50, Batch: 300, Loss: 1.9587478888034822\n",
      "Epoch: 24/50, Batch: 400, Loss: 1.9713350319862366\n",
      "Epoch: 24/50, Batch: 500, Loss: 2.024083753824234\n",
      "Epoch: 24/50, Batch: 600, Loss: 2.049364813566208\n",
      "Epoch: 24/50, Batch: 700, Loss: 2.0657480013370515\n",
      "Epoch: 24/50, Batch: 800, Loss: 2.103039363622665\n",
      "Epoch: 24/50, Batch: 900, Loss: 2.0931658804416657\n",
      "Epoch: 24/50, Batch: 1000, Loss: 2.1178281795978546\n",
      "Epoch: 24/50, Batch: 1100, Loss: 2.1430294132232666\n",
      "Epoch: 24/50, Batch: 1200, Loss: 2.165724891424179\n",
      "Epoch: 24/50, Batch: 1300, Loss: 2.173290101289749\n",
      "Epoch: 24/50, Batch: 1400, Loss: 2.196698613166809\n",
      "Epoch: 24/50, Batch: 1500, Loss: 2.211806811094284\n",
      "Epoch: 24/50, Batch: 1600, Loss: 2.2323673927783965\n",
      "Epoch: 24/50, Batch: 1700, Loss: 2.244366651773453\n",
      "Epoch: 24/50, Batch: 1800, Loss: 2.263727459907532\n",
      "Epoch: 24/50, Batch: 1900, Loss: 2.267863576412201\n",
      "Epoch: 25/50, Batch: 100, Loss: 1.8896536510602562\n",
      "Epoch: 25/50, Batch: 200, Loss: 1.8422799909114838\n",
      "Epoch: 25/50, Batch: 300, Loss: 1.895099093914032\n",
      "Epoch: 25/50, Batch: 400, Loss: 1.9103049945831299\n",
      "Epoch: 25/50, Batch: 500, Loss: 1.958350509405136\n",
      "Epoch: 25/50, Batch: 600, Loss: 1.9700106704235076\n",
      "Epoch: 25/50, Batch: 700, Loss: 2.0163410019874575\n",
      "Epoch: 25/50, Batch: 800, Loss: 1.9918440210819244\n",
      "Epoch: 25/50, Batch: 900, Loss: 2.00773398399353\n",
      "Epoch: 25/50, Batch: 1000, Loss: 2.07228897690773\n",
      "Epoch: 25/50, Batch: 1100, Loss: 2.094734027385712\n",
      "Epoch: 25/50, Batch: 1200, Loss: 2.119146245718002\n",
      "Epoch: 25/50, Batch: 1300, Loss: 2.1066630017757415\n",
      "Epoch: 25/50, Batch: 1400, Loss: 2.1493954503536226\n",
      "Epoch: 25/50, Batch: 1500, Loss: 2.163323827981949\n",
      "Epoch: 25/50, Batch: 1600, Loss: 2.178678742647171\n",
      "Epoch: 25/50, Batch: 1700, Loss: 2.1669912946224215\n",
      "Epoch: 25/50, Batch: 1800, Loss: 2.205748406648636\n",
      "Epoch: 25/50, Batch: 1900, Loss: 2.229549516439438\n",
      "Epoch: 26/50, Batch: 100, Loss: 1.843884401616797\n",
      "Epoch: 26/50, Batch: 200, Loss: 1.7695119392871856\n",
      "Epoch: 26/50, Batch: 300, Loss: 1.8399420332908631\n",
      "Epoch: 26/50, Batch: 400, Loss: 1.8520147752761842\n",
      "Epoch: 26/50, Batch: 500, Loss: 1.9034044826030732\n",
      "Epoch: 26/50, Batch: 600, Loss: 1.8903050899505616\n",
      "Epoch: 26/50, Batch: 700, Loss: 1.9202798891067505\n",
      "Epoch: 26/50, Batch: 800, Loss: 1.9673286664485932\n",
      "Epoch: 26/50, Batch: 900, Loss: 1.9894812548160552\n",
      "Epoch: 26/50, Batch: 1000, Loss: 1.9838766133785248\n",
      "Epoch: 26/50, Batch: 1100, Loss: 2.0185645604133606\n",
      "Epoch: 26/50, Batch: 1200, Loss: 2.0427487516403198\n",
      "Epoch: 26/50, Batch: 1300, Loss: 2.074745203256607\n",
      "Epoch: 26/50, Batch: 1400, Loss: 2.0546951413154604\n",
      "Epoch: 26/50, Batch: 1500, Loss: 2.084024440050125\n",
      "Epoch: 26/50, Batch: 1600, Loss: 2.0827435731887816\n",
      "Epoch: 26/50, Batch: 1700, Loss: 2.124208256006241\n",
      "Epoch: 26/50, Batch: 1800, Loss: 2.130143315792084\n",
      "Epoch: 26/50, Batch: 1900, Loss: 2.148500930070877\n",
      "Epoch: 27/50, Batch: 100, Loss: 1.8034479101147272\n",
      "Epoch: 27/50, Batch: 200, Loss: 1.7244377982616426\n",
      "Epoch: 27/50, Batch: 300, Loss: 1.734244645833969\n",
      "Epoch: 27/50, Batch: 400, Loss: 1.7619066643714905\n",
      "Epoch: 27/50, Batch: 500, Loss: 1.8117834496498109\n",
      "Epoch: 27/50, Batch: 600, Loss: 1.8583095622062684\n",
      "Epoch: 27/50, Batch: 700, Loss: 1.9000380194187165\n",
      "Epoch: 27/50, Batch: 800, Loss: 1.9189316821098328\n",
      "Epoch: 27/50, Batch: 900, Loss: 1.8822342658042908\n",
      "Epoch: 27/50, Batch: 1000, Loss: 1.942695314884186\n",
      "Epoch: 27/50, Batch: 1100, Loss: 1.9683082377910615\n",
      "Epoch: 27/50, Batch: 1200, Loss: 1.9602367436885835\n",
      "Epoch: 27/50, Batch: 1300, Loss: 2.0030486571788786\n",
      "Epoch: 27/50, Batch: 1400, Loss: 1.9978307104110717\n",
      "Epoch: 27/50, Batch: 1500, Loss: 2.0354600882530214\n",
      "Epoch: 27/50, Batch: 1600, Loss: 2.0776571106910704\n",
      "Epoch: 27/50, Batch: 1700, Loss: 2.038270684480667\n",
      "Epoch: 27/50, Batch: 1800, Loss: 2.0680071330070495\n",
      "Epoch: 27/50, Batch: 1900, Loss: 2.0983278489112855\n",
      "Epoch: 28/50, Batch: 100, Loss: 1.7219481193913824\n",
      "Epoch: 28/50, Batch: 200, Loss: 1.6960078227519988\n",
      "Epoch: 28/50, Batch: 300, Loss: 1.7224929666519164\n",
      "Epoch: 28/50, Batch: 400, Loss: 1.771417303085327\n",
      "Epoch: 28/50, Batch: 500, Loss: 1.7682257044315337\n",
      "Epoch: 28/50, Batch: 600, Loss: 1.7870933961868287\n",
      "Epoch: 28/50, Batch: 700, Loss: 1.830307354927063\n",
      "Epoch: 28/50, Batch: 800, Loss: 1.8146718335151673\n",
      "Epoch: 28/50, Batch: 900, Loss: 1.8810814583301545\n",
      "Epoch: 28/50, Batch: 1000, Loss: 1.9066312003135681\n",
      "Epoch: 28/50, Batch: 1100, Loss: 1.8667497086524962\n",
      "Epoch: 28/50, Batch: 1200, Loss: 1.916023691892624\n",
      "Epoch: 28/50, Batch: 1300, Loss: 1.9040771007537842\n",
      "Epoch: 28/50, Batch: 1400, Loss: 1.93209117770195\n",
      "Epoch: 28/50, Batch: 1500, Loss: 1.9646285843849183\n",
      "Epoch: 28/50, Batch: 1600, Loss: 1.9924116480350493\n",
      "Epoch: 28/50, Batch: 1700, Loss: 2.0332475101947782\n",
      "Epoch: 28/50, Batch: 1800, Loss: 2.005333594083786\n",
      "Epoch: 28/50, Batch: 1900, Loss: 2.0848577225208285\n",
      "Epoch: 29/50, Batch: 100, Loss: 1.6421150253937307\n",
      "Epoch: 29/50, Batch: 200, Loss: 1.637738198041916\n",
      "Epoch: 29/50, Batch: 300, Loss: 1.669405333995819\n",
      "Epoch: 29/50, Batch: 400, Loss: 1.6717492508888245\n",
      "Epoch: 29/50, Batch: 500, Loss: 1.711226511001587\n",
      "Epoch: 29/50, Batch: 600, Loss: 1.7310170578956603\n",
      "Epoch: 29/50, Batch: 700, Loss: 1.7833945453166962\n",
      "Epoch: 29/50, Batch: 800, Loss: 1.7970257520675659\n",
      "Epoch: 29/50, Batch: 900, Loss: 1.816632261276245\n",
      "Epoch: 29/50, Batch: 1000, Loss: 1.8462934303283691\n",
      "Epoch: 29/50, Batch: 1100, Loss: 1.8284556782245636\n",
      "Epoch: 29/50, Batch: 1200, Loss: 1.8918574464321136\n",
      "Epoch: 29/50, Batch: 1300, Loss: 1.8820517909526826\n",
      "Epoch: 29/50, Batch: 1400, Loss: 1.9259855711460114\n",
      "Epoch: 29/50, Batch: 1500, Loss: 1.9138718402385713\n",
      "Epoch: 29/50, Batch: 1600, Loss: 1.9011630237102508\n",
      "Epoch: 29/50, Batch: 1700, Loss: 1.9580919361114502\n",
      "Epoch: 29/50, Batch: 1800, Loss: 1.984065853357315\n",
      "Epoch: 29/50, Batch: 1900, Loss: 1.9494808435440063\n",
      "Epoch: 30/50, Batch: 100, Loss: 1.61727001076251\n",
      "Epoch: 30/50, Batch: 200, Loss: 1.5897388410568238\n",
      "Epoch: 30/50, Batch: 300, Loss: 1.5816826462745666\n",
      "Epoch: 30/50, Batch: 400, Loss: 1.6082445621490478\n",
      "Epoch: 30/50, Batch: 500, Loss: 1.608200922012329\n",
      "Epoch: 30/50, Batch: 600, Loss: 1.6758486354351043\n",
      "Epoch: 30/50, Batch: 700, Loss: 1.6905000412464142\n",
      "Epoch: 30/50, Batch: 800, Loss: 1.7355234694480897\n",
      "Epoch: 30/50, Batch: 900, Loss: 1.7231394910812379\n",
      "Epoch: 30/50, Batch: 1000, Loss: 1.7816966438293458\n",
      "Epoch: 30/50, Batch: 1100, Loss: 1.774488229751587\n",
      "Epoch: 30/50, Batch: 1200, Loss: 1.8155972564220428\n",
      "Epoch: 30/50, Batch: 1300, Loss: 1.8462098705768586\n",
      "Epoch: 30/50, Batch: 1400, Loss: 1.8510283207893372\n",
      "Epoch: 30/50, Batch: 1500, Loss: 1.8780477035045624\n",
      "Epoch: 32/50, Batch: 900, Loss: 1.6623314011096955\n",
      "Epoch: 32/50, Batch: 1000, Loss: 1.7210777497291565\n",
      "Epoch: 32/50, Batch: 1100, Loss: 1.7159018349647521\n",
      "Epoch: 32/50, Batch: 1200, Loss: 1.7200209677219391\n",
      "Epoch: 32/50, Batch: 1300, Loss: 1.7515731394290923\n",
      "Epoch: 32/50, Batch: 1400, Loss: 1.7820039367675782\n",
      "Epoch: 32/50, Batch: 1500, Loss: 1.7561214935779572\n",
      "Epoch: 32/50, Batch: 1600, Loss: 1.7864222836494446\n",
      "Epoch: 32/50, Batch: 1700, Loss: 1.805014660358429\n",
      "Epoch: 32/50, Batch: 1800, Loss: 1.8324480032920838\n",
      "Epoch: 32/50, Batch: 1900, Loss: 1.8158547270298004\n",
      "Epoch: 33/50, Batch: 100, Loss: 1.498258432983297\n",
      "Epoch: 33/50, Batch: 200, Loss: 1.4606069350242614\n",
      "Epoch: 33/50, Batch: 300, Loss: 1.4344004786014557\n",
      "Epoch: 33/50, Batch: 400, Loss: 1.4927971458435059\n",
      "Epoch: 33/50, Batch: 500, Loss: 1.5027126908302306\n",
      "Epoch: 33/50, Batch: 600, Loss: 1.575650624036789\n",
      "Epoch: 33/50, Batch: 700, Loss: 1.5829326653480529\n",
      "Epoch: 33/50, Batch: 800, Loss: 1.594592765569687\n",
      "Epoch: 33/50, Batch: 900, Loss: 1.6193406772613526\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sequence_length = 10\n",
    "batch_size = 128\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "output_size = vocab_size\n",
    "embedding_dim = 200\n",
    "hidden_dim = 250\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs)\n",
    "\n",
    "save_model('/kaggle/working/trained_rnn.pth', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate text ‚úçÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T18:25:11.843009Z",
     "iopub.status.busy": "2024-05-20T18:25:11.842392Z",
     "iopub.status.idle": "2024-05-20T18:25:14.671938Z",
     "shell.execute_reply": "2024-05-20T18:25:14.670789Z",
     "shell.execute_reply.started": "2024-05-20T18:25:11.842976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "i have to tell you something\n",
      ", ‚Äù razumihin said, and was not in his\n",
      "hand, as she had not seen that she is a minute,\n",
      "and i am a word, you are not a man... i shall\n",
      "be, but he is a minute of the same\n",
      "woman, ‚Äù said raskolnikov, but she was a man of\n",
      "the street and had been in the same man,\n",
      "and a man was the room of a word,\n",
      "and the man was in the room and the whole time.\n",
      "\n",
      "‚Äúi am not a man, and i won‚Äôt be a good man, but you are not\n",
      "very woman, ‚Äù he said and was a word, and she was a little man, and was in a minute.\n",
      "but it is to be not in the same thing\n",
      "of the whole moment... i won‚Äôt know,\n",
      "but it is, you have not not seen. i am a man\n",
      ", ‚Äù razumihin said to a man, but the\n",
      "door, as he had been a minute of her\n",
      ".\n",
      "\n",
      "‚Äúi are a woman, i am a minute, and it was\n",
      "the door, too......... i am very to be, i won‚Äôt\n",
      "know. you? what is it, ‚Äù cried razumihin, and he was not\n",
      "to the whole man, but it was a very woman of a\n",
      "man.\n",
      "\n",
      "‚Äúwhat, i am a minute, ‚Äù he said.\n",
      "\n",
      "‚Äúi don‚Äôt know, i shall have a whole man,\n",
      "i won‚Äôt think. ‚Äù\n",
      "\n",
      "‚Äúi am a man! ‚Äù razumihin asked and looked\n",
      "to the room.\n",
      "\n",
      "‚Äúi am not not a word.... i shall be\n",
      "a man, ‚Äù said razumihin.\n",
      "\n",
      "‚Äúi have a whole woman, ‚Äù he thought.\n",
      "\n",
      "‚Äúi have seen you! ‚Äù\n",
      "\n",
      "‚Äúi won‚Äôt be not a very woman of the same,\n",
      "and a minute and, he is a minute of\n",
      "a man. i am a man of the whole woman of the\n",
      "whole man, as she was a man, but\n",
      "he was not in the street. she has not not\n",
      "come to her, ‚Äù he said and had not not\n",
      "come, and he was a man, he was not a\n",
      "whole woman.\n",
      "\n",
      "‚Äúyes, you are a man. ‚Äù\n",
      "\n",
      "‚Äúyes. ‚Äù\n",
      "\n",
      "‚Äúi won‚Äôt be a woman, ‚Äù he thought.\n",
      "\n",
      "‚Äúwhat! ‚Äù\n",
      "\n",
      "‚Äúwhat, ‚Äù razumihin had not gone.\n",
      "\n",
      "‚Äúi am a man, ‚Äù said razumihin.\n",
      "\n",
      "‚Äúyes... i am not afraid of a man of\n",
      "a man, too.\n",
      "\n",
      "‚Äúwhat, i am not not not to you....\n",
      "but you know, i shall have been to be\n",
      "not the door..... and he had a\n",
      "woman, and i have not not a man,\n",
      "but i am a minute and not a man to\n",
      "speak of the room, and and a moment and\n",
      "and the man was not a man.... ‚Äù\n",
      "\n",
      "‚Äúyes, ‚Äù he asked, as the street was\n",
      "a little woman with his face, and and and he had\n",
      "a woman, as the corner. and she was to\n",
      "the whole moment of the same woman of the whole\n",
      "thing. ‚Äù\n",
      "\n",
      "‚Äúyou are going. i am not a whole time! but i am a man, and\n",
      "it was a man of a moment, and that the\n",
      "woman is to you. i will know that you are a\n",
      "man and a man, and i am a man,\n",
      "and you, and you were to be, and\n",
      "he is to be a man.... but you are\n",
      "a whole woman, ‚Äù he asked, but he had been\n",
      "in a word. the whole woman was in her\n",
      "face, he was a minute, and she would be\n",
      "to be and the street, and he would come to\n",
      "you... and i won‚Äôt have been a\n",
      "whole time... i am not not not a man of the\n",
      "whole way, but it had been to the street, he was to the\n",
      "whole woman and the street and the whole time was\n",
      "in a minute of the whole way and, but the\n",
      "whole time, as though she was a man. she is\n",
      "a man and he was a woman, and she had a\n",
      "man and he had not not been in a man\n",
      "and the whole woman and he had been in a woman\n",
      ", but he is a word. ‚Äù\n",
      "\n",
      "‚Äúi have gone in that, ‚Äù he asked, and\n",
      "the same woman and looked, he was a woman.\n",
      "\n",
      "‚Äúyes, ‚Äù\n",
      "\n",
      "‚Äúyou? ‚Äù\n",
      "\n",
      "‚Äúyou have been a very thing? ‚Äù\n",
      "\n",
      "‚Äúyes. you are not to you, ‚Äù he said\n",
      "on, as she were and a man and was not\n",
      "not and a man,\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def load_model(file_path, model):\n",
    "    model.load_state_dict(torch.load(file_path))\n",
    "    model.eval()\n",
    "\n",
    "def generate(rnn, prime_ids, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    prime_len = len(prime_ids)\n",
    "    current_seq[0, -prime_len:] = prime_ids\n",
    "    predicted = [int_to_vocab[idx] for idx in prime_ids]\n",
    "    hidden = rnn.init_hidden(1)\n",
    "\n",
    "    for _ in range(predict_len):\n",
    "        current_seq = torch.LongTensor(current_seq).to(device)\n",
    "        output, hidden = rnn(current_seq, hidden)\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if train_on_gpu:\n",
    "            p = p.cpu()\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)\n",
    "        current_seq = np.roll(current_seq.cpu(), -1, 1)\n",
    "        current_seq[0, -1] = word_i\n",
    "\n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    return gen_sentences\n",
    "\n",
    "# Load the trained model\n",
    "rnn_loaded = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout)\n",
    "if train_on_gpu:\n",
    "    rnn_loaded.cuda()\n",
    "\n",
    "load_model('/kaggle/working/trained_rnn.pth', rnn_loaded)\n",
    "print('Model Loaded')\n",
    "\n",
    "# Generate text\n",
    "gen_length = 1000\n",
    "prime_phrase = 'i have to tell you something'\n",
    "\n",
    "pad_word = SPECIAL_WORDS['PADDING']\n",
    "prime_ids = [vocab_to_int[word] for word in prime_phrase.split() if word in vocab_to_int]\n",
    "generated_script = generate(rnn_loaded, prime_ids, int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the model üìä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19552\\57486223.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrouge_score\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrouge_scorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# these two unused imports are referenced in collocations.doctest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m from nltk.metrics import (\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magreement\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAnnotationTask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m from nltk.metrics.association import (\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\metrics\\association.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfisher_exact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    465\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    466\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 467\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    468\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\stats\\_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_measurements\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m from scipy._lib._util import (check_random_state, MapWrapper,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \"\"\"\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_kdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_qhull\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\spatial\\_kdtree.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_ckdtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcKDTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcKDTreeNode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m __all__ = ['minkowski_distance_p', 'minkowski_distance',\n",
      "\u001b[1;32m_ckdtree.pyx\u001b[0m in \u001b[0;36minit scipy.spatial._ckdtree\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_matrix_io\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m from ._arrays import (\n\u001b[0m\u001b[0;32m    279\u001b[0m     \u001b[0mcsr_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsc_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlil_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdok_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoo_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdia_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbsr_array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "from collections import Counter\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "# turn punctuation into a token\n",
    "def token_lookup():\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotation_mark||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation_mark||',\n",
    "        '?': '||question_mark||',\n",
    "        '(': '||left_parenthesis||',\n",
    "        ')': '||right_parenthesis||',\n",
    "        '--': '||dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n",
    "\n",
    "def reverse_tokenize(text, token_dict):\n",
    "    for key, token in token_dict.items():\n",
    "        text = text.replace(token.lower(), key)\n",
    "    return text\n",
    "\n",
    "dataset_path = '/kaggle/input/crime-and-punishment/cap.txt'\n",
    "dataset = load_data(dataset_path)\n",
    "\n",
    "# Tokenization\n",
    "token_dict = token_lookup()\n",
    "for key, token in token_dict.items():\n",
    "    dataset = dataset.replace(key, f' {token} ')\n",
    "\n",
    "dataset = dataset.lower()\n",
    "\n",
    "sentences = dataset.split(' ||period|| ') \n",
    "\n",
    "reference_texts = sentences[100:200]  \n",
    "\n",
    "# Reverse tokenization for reference texts\n",
    "reference_texts = [reverse_tokenize(ref, token_dict) for ref in reference_texts]\n",
    "\n",
    "generated_script = generate(rnn_loaded, prime_ids, int_to_vocab, token_dict, vocab_to_int[SPECIAL_WORDS['PADDING']], gen_length)\n",
    "\n",
    "# Reverse tokenization for generated text\n",
    "generated_script = reverse_tokenize(generated_script, token_dict)\n",
    "\n",
    "# Tokenize the generated text\n",
    "generated_tokens = generated_script.split()\n",
    "\n",
    "# Tokenize reference texts\n",
    "reference_tokens = [ref.split() for ref in reference_texts]\n",
    "\n",
    "# BLEU score\n",
    "bleu_score = sentence_bleu(reference_tokens, generated_tokens)\n",
    "print(\"BLEU Score:\", bleu_score)\n",
    "\n",
    "#ROUGE score\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(' '.join(reference_texts), generated_script)\n",
    "print(\"ROUGE-1 Score:\", rouge_scores['rouge1'])\n",
    "print(\"ROUGE-L Score:\", rouge_scores['rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5050176,
     "sourceId": 8469773,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
